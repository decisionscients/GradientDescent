Automatically generated by Mendeley Desktop 1.19.4
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@techreport{Daniel,
author = {Daniel, Jurafsky and Martin, James H},
file = {::},
title = {{Speech and Language Processing}}
}
@techreport{Mcallester2000,
abstract = {Good-Turing adjustments of word frequencies are an important tool in natural language modeling. In particular, for any sample of words, there is a set of words not occuring in that sample. The total probability mass of the words not in the sample is the so-called missing mass. Good showed that the fraction of the sample consisting of words that occur only once in the sample is a nearly unbiased estimate of the missing mass. Here, we give a high-probability confidence interval for the actual missing mass. More generally, for k 0, we give a confidence interval for the true probability mass of the set of words occuring k times in the sample.},
author = {Mcallester, David and Schapire, Robert E},
file = {::},
title = {{On the Convergence Rate of Good-Turing Estimators}},
url = {http://rob.schapire.net/papers/good-turing.pdf},
year = {2000}
}
@article{Dick2013,
abstract = {Many machine learning problems, such as K-means, are non-convex optimization problems. Usually they are solved by performing several local searches with ran-dom initializations. How many searches should be done? Typically a fixed num-ber is performed, but how do we know it was enough? We present a new stopping rule with non-asymptotic frequentist guarantees, which, to our knowledge, no ex-isting rule has. By comparing all stopping rules on various benchmarks, we shed light on their effectiveness in machine-learning problems, including K-means and maximum marginal likelihood parameter selection.},
author = {Dick, Travis and Wong, Eric and Dann, Christoph},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Dick, Wong, Dann - 2013 - How many random restarts are enough.pdf:pdf},
title = {{How many random restarts are enough ?}},
url = {https://www.cs.cmu.edu/{~}epxing/Class/10715/project-reports/DannDickWong.pdf},
year = {2013}
}
@techreport{Marti2016,
abstract = {Heuristic search procedures aimed at finding globally optimal solutions to hard combinatorial optimization problems usually require some type of diversification to overcome local optimality. One way to achieve diversification is to restart the procedure from a new solution once a region has been explored, which constitutes a multi-start procedure. In this chapter we describe the best known multi-start methods for solving optimization problems. We also describe their connections with other metaheuristic methodologies. We propose classifying these methods in terms of their use of randomization, memory and degree of rebuild. We also present a computational comparison of these methods on solving the Maximum Diversity Problem to illustrate the efficiency of the multi-start methodology in terms of solution quality and diversification power.},
author = {Mart{\'{i}}, R and Aceves, R and Le{\'{o}}n, M T and Moreno-Vega, J M and Duarte, A and Mart{\'{i}}, Rafael and Aceves, Ricardo and Le{\'{o}}n, Maria Teresa and Moreno-Vega, Jose Marcos and Duarte, Abraham},
file = {::},
title = {{Intelligent Multi-Start Methods}},
url = {https://www.uv.es/rmarti/paper/docs/multi3.pdf}
}
@misc{wiki:Differentiable,
author = {{Wikipedia Contributors}},
title = {{Differentiable Function}}
}
@misc{tangent,
annote = {[Online; accessed 25-October-2018]},
author = {Wikipedia contributors},
title = {{Tangent --- Wikipedia -The Free Encyclopedia}},
url = {https://en.wikipedia.org/w/index.php?title=Tangent{\&}oldid=848826334},
year = {2018}
}
@misc{mitchell1997machine,
author = {Mitchell, Tom M and Others},
publisher = {McGraw-Hill Boston, MA:},
title = {{Machine learning. WCB}},
year = {1997}
}
@book{Mitchell1997,
abstract = {Mitchell covers the field of machine learning, the study of algorithms that allow computer programs to automatically improve through experience and that automatically infer general laws from specific data. 1. Introduction -- 2. Concept Learning and the General-to-Specific Ordering -- 3. Decision Tree Learning -- 4. Artificial Neural Networks -- 5. Evaluating Hypotheses -- 6. Bayesian Learning -- 7. Computational Learning Theory -- 8. Instance-Based Learning -- 9. Genetic Algorithms -- 10. Learning Sets of Rules -- 11. Analytical Learning -- 12. Combining Inductive and Analytical Learning -- 13. Reinforcement Learning.},
author = {Mitchell, Tom M. (Tom Michael)},
isbn = {0070428077},
pages = {414},
publisher = {McGraw-Hill},
title = {{Machine Learning}},
url = {http://www.cs.cmu.edu/{~}tom/mlbook.html},
year = {1997}
}
@article{Hinton2012,
abstract = {Tieleman, Tijmen and Hinton, Geoffrey. Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude. COURSERA: Neural Networks for Machine Learning, 4, 2012},
author = {Hinton, Geoffrey E. and Srivastava, Nitish and Swersky, Kevin},
file = {::},
journal = {COURSERA: Neural Networks for Machine Learning},
pages = {29},
title = {{Neural Networks for Machine Learning Lecture 6a Overview of mini-batch gradient descent.}},
url = {https://www.cs.toronto.edu/{~}tijmen/csc321/slides/lecture{\_}slides{\_}lec6.pdf http://www.cs.toronto.edu/{~}tijmen/csc321/slides/lecture{\_}slides{\_}lec6.pdf},
year = {2012}
}
@article{Robbins1951,
abstract = {Let M(x) denote the expected value at level x of the response to a certain experiment. M(x) is assumed to be a monotone function of x but is unknown to the experimenter, and it is desired to find the solution x = $\theta$ of the equation M(x) = $\alpha$, where $\alpha$ is a given constant. We give a method for making successive experiments at levels x1,x2,⋯ in such a way that xn will tend to $\theta$ in probability.},
author = {Robbins, Herbert and Monro, Sutton},
doi = {10.1214/aoms/1177729586},
file = {::},
isbn = {0003-4851},
issn = {0003-4851},
journal = {The Annals of Mathematical Statistics},
number = {3},
pages = {400--407},
title = {{A Stochastic Approximation Method}},
url = {https://projecteuclid.org/download/pdf{\_}1/euclid.aoms/1177729586 http://projecteuclid.org/euclid.aoms/1177729586{\%}5Cnpapers2://publication/uuid/0E522956-F1DE-4A56-885A-E780F05A8297 http://projecteuclid.org/euclid.aoms/1177729586},
volume = {22},
year = {1951}
}
@article{Ravauta,
abstract = {Any gradient descent requires to choose a learning rate. With deeper and deeper models, tuning that learning rate can easily become tedious and does not necesarily lead to an ideal convergence. We propose a variation of the gradient descent algorithm in the which the learning rate $\eta$ is not fixed. Instead, we learn $\eta$ itself, either by another gradient descent (first-order method), or by Newton's method (second-order). This way, gradient descent for any machine learning algorithm can be optimized.},
author = {Ravaut, Mathieu},
file = {::},
title = {{Faster gradient descent via an adaptive learning rate}},
url = {http://www.cs.toronto.edu/{~}mravox/p4.pdf}
}
@inproceedings{paszke2017automatic,
author = {Paszke, Adam and Gross, Sam and Chintala, Soumith and Chanan, Gregory and Yang, Edward and DeVito, Zachary and Lin, Zeming and Desmaison, Alban and Antiga, Luca and Lerer, Adam},
booktitle = {NIPS-W},
title = {{Automatic differentiation in PyTorch}},
year = {2017}
}
@misc{gradient,
annote = {[Online; accessed 25-October-2018]},
author = {Wikipedia},
title = {{Gradient - Wikipedia}},
url = {https://en.wikipedia.org/w/index.php?title=Camera{\_}obscura{\&}oldid=862875479},
year = {2018}
}
@inproceedings{1716540,
abstract = {The standard method in optimization problems consists in a random search of the global minimum: a neuron network relaxes in the nearest local minimum from some randomly chosen initial configuration. This procedure is to be repeated many times in order to find as deep energy minimum as possible. However the question about the reasonable number of such random starts and if the result of the search can be treated as successful remains always open. In this paper by analyzing the generalized Hopfield model we obtain expressions, which yield the relationship between the depth of a local minimum and the size of the basin of attraction. Based on this, we present the probability of finding a local minimum as a function of the depth of the minimum. Such a relation can be used in optimization applications: it allows one, basing on a series of already found minima, to estimate the probability of finding a deeper minimum, and decide in favor of or against further running the program. The theory is in a good agreement with experimental results.},
author = {Kryzhanovsky, Boris and Magomedov, Bashir and Fonarev, Anatoly},
booktitle = {The 2006 IEEE International Joint Conference on Neural Network Proceedings},
doi = {10.1109/IJCNN.2006.247318},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kryzhanovsky, Magomedov, Fonarev - Unknown - On the Probability of Finding Local Minima in Optimization Problems.pdf:pdf},
issn = {2161-4393},
keywords = {Hopfield neural nets,optimisation,probability,rand},
month = {jul},
pages = {3243--3248},
title = {{On the Probability of Finding Local Minima in Optimization Problems}},
url = {http://csivc.csi.cuny.edu/Anatoly.Fonarev/files/me{\_}scient/Kryzhmag{\_}On{\_}the{\_}probability{\%}5B1{\%}5D.pdf},
year = {2006}
}
@misc{tensorflow2015-whitepaper,
annote = {Software available from tensorflow.org},
author = {Martin{\~{}}Abadi and Ashish{\~{}}Agarwal and Paul{\~{}}Barham and Eugene{\~{}}Brevdo and Zhifeng{\~{}}Chen and Craig{\~{}}Citro and Greg{\~{}}S.{\~{}}Corrado and Andy{\~{}}Davis and Jeffrey{\~{}}Dean and Matthieu{\~{}}Devin and Sanjay{\~{}}Ghemawat and Ian{\~{}}Goodfellow and Andrew{\~{}}Harp and Geoffrey{\~{}}Irving and Michael{\~{}}Isard and Jia, Yangqing and Rafal{\~{}}Jozefowicz and Lukasz{\~{}}Kaiser and Manjunath{\~{}}Kudlur and Josh{\~{}}Levenberg and Dandelion{\~{}}Man{\'{e}} and Rajat{\~{}}Monga and Sherry{\~{}}Moore and Derek{\~{}}Murray and Chris{\~{}}Olah and Mike{\~{}}Schuster and Jonathon{\~{}}Shlens and Benoit{\~{}}Steiner and Ilya{\~{}}Sutskever and Kunal{\~{}}Talwar and Paul{\~{}}Tucker and Vincent{\~{}}Vanhoucke and Vijay{\~{}}Vasudevan and Fernanda{\~{}}Vi{\'{e}}gas and Oriol{\~{}}Vinyals and Pete{\~{}}Warden and Martin{\~{}}Wattenberg and Martin{\~{}}Wicke and Yuan{\~{}}Yu and Xiaoqiang{\~{}}Zheng},
title = {{TensorFlow: Large-Scale Machine Learning on Heterogeneous Systems}},
url = {https://www.tensorflow.org/},
year = {2015}
}
@inproceedings{DBLP:journals/corr/ZhangCL14a,
author = {Zhang, Sixin and Choromanska, Anna and LeCun, Yann},
booktitle = {{\{}ICLR{\}} (Workshop)},
title = {{Deep learning with Elastic Averaging {\{}SGD{\}}}},
url = {http://arxiv.org/abs/1412.6651},
year = {2015}
}
@article{Cauchy1847Methode,
author = {Cauchy, Augustin-Louis},
journal = {Compte Rendu des S'eances de L'Acad'emie des Sciences XXV},
keywords = {first,steepest-descent},
month = {oct},
number = {25},
pages = {536--538},
title = {{M{\'{e}}thode g{\'{e}}n{\'{e}}rale pour la r{\'{e}}solution des syst{\`{e}}mes d'{\'{e}}quations simultan{\'{e}}es}},
volume = {S'erie A},
year = {1847}
}
@article{facebook,
author = {Singh, Kamaljot},
doi = {10.5013/IJSSST.a.16.05.16},
journal = {International Journal of Simulation: Systems, Science {\&} Technology},
pages = {16.1},
title = {{Facebook comment volume prediction}},
volume = {16},
year = {2015}
}
@misc{KaggleInc2016,
author = {{Kaggle Inc}},
booktitle = {kaggle.com},
title = {{House Prices: Advanced Regression Techniques - Prizes}},
url = {https://www.kaggle.com/c/house-prices-advanced-regression-techniques https://www.kaggle.com/c/house-prices-advanced-regression-techniques/details/prizes},
urldate = {2018-10-29},
year = {2016}
}
@incollection{Prechelt2012,
abstract = {Validation can be used to detect when overrtting starts during supervised training of a neural network; training is then stopped before convergence to avoid the overrtting early stopping". The exact criterion used for validation-based early stopping, however, is usually chosen in an ad-hoc fashion or training is stopped interactively. This trick describes how to select a stopping criterion in a systematic fashion; it is a trick for either speeding learning procedures or improving generalization , whichever is more important in the particular situation. An empirical investigation on multi-layer perceptrons shows that there exists a tradeoo between training time and generalization: From the given mix of 1296 training runs using diierent 12 problems and 24 diierent network architectures I conclude slower stopping criteria allow for small improvements in generalization here: about 4 on average, but cost much more training time here: about factor 4 longer on average.},
archivePrefix = {arXiv},
arxivId = {1412.6830},
author = {Prechelt, Lutz},
doi = {10.1007/978-3-642-35289-8_5},
eprint = {1412.6830},
file = {::},
isbn = {9783642352881},
issn = {01628828},
pages = {53--67},
pmid = {8514134},
title = {{Early Stopping — But When?}},
url = {https://page.mi.fu-berlin.de/prechelt/Biblio/stop{\_}tricks1997.pdf http://link.springer.com/10.1007/978-3-642-35289-8{\_}5},
year = {2012}
}
@misc{wiki:unsupervisedlearning,
annote = {[Online; accessed 
8-September-2017
]},
author = {Wikipedia},
title = {{Unsupervised learning --- Wikipedia{\{},{\}} The Free Encyclopedia}},
url = {https://en.wikipedia.org/w/index.php?title=Unsupervised{\_}learning{\&}oldid=793838440},
year = {2017}
}
@misc{UniversityofWash,
abstract = {This Specialization from leading researchers at the University of Washington introduces you to the exciting, high-demand field of Machine Learning. Through a series of practical case studies, you will gain applied experience in major areas of Machine Learning including Prediction, Classification, Clustering, and Information Retrieval. You will learn to analyze large and complex datasets, create systems that adapt and improve over time, and build intelligent applications that can make predictions from data.},
author = {{University of Washington}},
keywords = {Coursera,certificates,courses,education,free,mooc,online,specializations},
title = {{Machine Learning - University of Washington | Coursera}},
url = {https://www.coursera.org/specializations/machine-learning https://www.coursera.org/course/machlearning},
urldate = {2017-09-08}
}
@book{strang09,
abstract = {Book Description: Gilbert Strang's textbooks have changed the entire approach to learning linear algebra -- away from abstract vector spaces to specific examples of the four fundamental subspaces: the column space and nullspace of A and A'. Introduction to Linear Algebra, Fourth Edition includes challenge problems to complement the review problems that have been highly praised in previous editions. The basic course is followed by seven applications: differential equations, engineering, graph theory, statistics, Fourier methods and the FFT, linear programming, and computer graphics. Thousands of teachers in colleges and universities and now high schools are using this book, which truly explains this crucial subject.},
address = {Wellesley, MA},
author = {Strang, Gilbert},
edition = {Fourth},
isbn = {9780980232714 0980232716 9780980232721 0980232724 9788175968110 8175968117},
keywords = {linear.algebra matrix strang textbook},
publisher = {Wellesley-Cambridge Press},
title = {{Introduction to Linear Algebra}},
year = {2009}
}
@article{scikit-learn,
author = {Pedregosa, F and Varoquaux, G and Gramfort, A and Michel, V and Thirion, B and Grisel, O and Blondel, M and Prettenhofer, P and Weiss, R and Dubourg, V and Vanderplas, J and Passos, A and Cournapeau, D and Brucher, M and Perrot, M and Duchesnay, E},
journal = {Journal of Machine Learning Research},
pages = {2825--2830},
title = {{Scikit-learn: Machine Learning in Python}},
volume = {12},
year = {2011}
}
@misc{TheMathworksInc.2016,
abstract = {The MATLAB platform is optimized for solving engineering and scientific problems. The matrix-based MATLAB language is the world's most natural way to express computational mathematics.},
author = {{The Mathworks Inc.}},
booktitle = {www.mathworks.com/products/matlab},
doi = {2016-11-26},
title = {{MATLAB - MathWorks}},
url = {https://www.mathworks.com/products/matlab.html http://www.mathworks.com/products/matlab/},
urldate = {2017-09-16},
year = {2016}
}
@article{Keskar2016,
abstract = {The stochastic gradient descent (SGD) method and its variants are algorithms of choice for many Deep Learning tasks. These methods operate in a small-batch regime wherein a fraction of the training data, say {\$}32{\$}-{\$}512{\$} data points, is sampled to compute an approximation to the gradient. It has been observed in practice that when using a larger batch there is a degradation in the quality of the model, as measured by its ability to generalize. We investigate the cause for this generalization drop in the large-batch regime and present numerical evidence that supports the view that large-batch methods tend to converge to sharp minimizers of the training and testing functions - and as is well known, sharp minima lead to poorer generalization. In contrast, small-batch methods consistently converge to flat minimizers, and our experiments support a commonly held view that this is due to the inherent noise in the gradient estimation. We discuss several strategies to attempt to help large-batch methods eliminate this generalization gap.},
archivePrefix = {arXiv},
arxivId = {1609.04836},
author = {Keskar, Nitish Shirish and Mudigere, Dheevatsa and Nocedal, Jorge and Smelyanskiy, Mikhail and Tang, Ping Tak Peter},
doi = {10.1227/01.NEU.0000255452.20602.C9},
eprint = {1609.04836},
file = {::},
isbn = {9781405161251},
issn = {0148396X},
pmid = {17460516},
title = {{On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima}},
url = {https://arxiv.org/pdf/1609.04836.pdf http://arxiv.org/abs/1609.04836},
year = {2016}
}
@article{Reiss2019,
abstract = {Photoplethysmography (PPG)-based continuous heart rate monitoring is essential in a number of domains, e.g., for healthcare or fitness applications. Recently, methods based on time-frequency spectra emerged to address the challenges of motion artefact compensation. However, existing approaches are highly parametrised and optimised for specific scenarios of small, public datasets. We address this fragmentation by contributing research into the robustness and generalisation capabilities of PPG-based heart rate estimation approaches. First, we introduce a novel large-scale dataset (called PPG-DaLiA), including a wide range of activities performed under close to real-life conditions. Second, we extend a state-of-the-art algorithm, significantly improving its performance on several datasets. Third, we introduce deep learning to this domain, and investigate various convolutional neural network architectures. Our end-to-end learning approach takes the time-frequency spectra of synchronised PPG- and accelerometer-signals as input, and provides the estimated heart rate as output. Finally, we compare the novel deep learning approach to classical methods, performing evaluation on four public datasets. We show that on large datasets the deep learning model significantly outperforms other methods: The mean absolute error could be reduced by 31 {\%} on the new dataset PPG-DaLiA, and by 21 {\%} on the dataset WESAD.},
author = {Reiss, Attila and Indlekofer, Ina and Schmidt, Philip and {Van Laerhoven}, Kristof},
doi = {10.3390/s19143079},
file = {::},
issn = {1424-8220},
journal = {Sensors},
month = {jul},
number = {14},
pages = {3079},
publisher = {MDPI AG},
title = {{Deep PPG: Large-Scale Heart Rate Estimation with Convolutional Neural Networks}},
volume = {19},
year = {2019}
}
@inproceedings{Caruana,
abstract = {In this paper we perform an empirical evaluation of supervised learning on high-dimensional data. We evaluate performance on three metrics: accuracy, AUC, and squared loss and study the effect of increasing dimensionality on the performance of the learning algorithms. Our findings are consistent with previous studies for problems of relatively low dimension, but suggest that as dimensionality increases the relative performance of the learning algorithms changes. To our surprise, the method that performs consistently well across all dimensions is random forests, followed by neural nets, boosted trees, and SVMs.},
archivePrefix = {arXiv},
arxivId = {cs/9605103},
author = {Caruana, Rich and Karampatziakis, Nikos and Yessenalina, Ainur},
booktitle = {Proceedings of the 25th international conference on Machine learning - ICML '08},
doi = {10.1145/1390156.1390169},
eprint = {9605103},
file = {::},
isbn = {9781605582054},
issn = {9781605582054},
pages = {96--103},
pmid = {17255001},
primaryClass = {cs},
title = {{An empirical evaluation of supervised learning in high dimensions}},
url = {http://yann.lecun.com/exdb/mnist/ http://portal.acm.org/citation.cfm?doid=1390156.1390169},
year = {2008}
}
@article{Samuel,
abstract = {Two machine-learning procedures have been investigated in some detail usi!Jg the game of checkers. Enough work has been done to verify the fact that a computer can be programmed so that it will learn to playa better game of checkers than can be played by the person who wrote the program. Further-more, it can learn to do this in a remarkably short period of time (8 or 10 hours of machine-playing time) when given only the rules of the game, a sense of direction, and a redundant and incomplete list of parameters which are thought to have something to do with the game, but whose correct signs and relative weights are unknown and unspecified. The principles of machine learning verified by these 'experiments are, of course, applicable to many other situations.},
author = {Samuel, Arthur L},
file = {::},
title = {{4.3.3 Some Studies in Machine Learning Using the Game of Checkers Some Studies in Machine Learning Using the Game of Checkers}},
url = {https://www.cs.virginia.edu/{~}evans/greatworks/samuel1959.pdf}
}
@book{strang2006linear,
address = {Belmont, CA},
author = {Strang, Gilbert},
isbn = {0030105676 9780030105678 0534422004 9780534422004},
keywords = {book math to{\_}READ},
publisher = {Thomson, Brooks/Cole},
title = {{Linear algebra and its applications}},
url = {http://www.amazon.com/Linear-Algebra-Its-Applications-Edition/dp/0030105676},
year = {2006}
}
@misc{Sarle2014,
abstract = {FAQ related to re-scaling/normalising NN (and other related machine learning methods) input data. Covers both "feature" and "case" based data re-scaling.},
author = {Sarle, Warren},
keywords = {SVM,SVM{\_}features,normalisation},
title = {{Should I normalize/standardize/rescale the}},
url = {http://www.faqs.org/faqs/ai-faq/neural-nets/part2/section-16.html},
urldate = {2018-10-06},
year = {2014}
}
@article{Harrison,
abstract = {This paper investigates the methodological problems associated with the use of housing market data to measure the willingness to pay for clean air. With the use of a hedonic housing price model and data for the Boston metropolitan area, quantitative estimates of the willingness to pay for air quality improvements are generated. Marginal air pollution damages (as revealed in the housing market) are found to increase with the level of air pollution and with household income. The results are relatively sensitive to the specification of the hedonic housing price equation, but insensitive to the specification of the air quality demand equation. {\textcopyright} 1978.},
author = {Harrison, David and Rubinfeld, Daniel L},
doi = {10.1016/0095-0696(78)90006-2},
file = {::},
issn = {10960449},
journal = {Journal of Environmental Economics and Management},
number = {1},
pages = {81--102},
title = {{Hedonic housing prices and the demand for clean air}},
volume = {5},
year = {1978}
}
@book{Data2007,
abstract = {This is an introductory course in machine learning (ML) that covers the basic theory, algorithms, and applications. ML is a key technology in Big Data, and in many financial, medical, commercial, and scientific applications. It enables computational systems to adaptively improve their performance with experience accumulated from the observed data. ML has become one of the hottest fields of study today, taken up by undergraduate and graduate students from 15 different majors at Caltech. This course balances theory and practice, and covers the mathematical as well as the heuristic aspects.},
author = {Data, Learning From},
isbn = {9780471681823},
keywords = {rning from data},
title = {{LEARNING FROM DATA}},
url = {https://work.caltech.edu/telecourse},
year = {2007}
}
@techreport{Wolpert1992,
abstract = {This paper pr oves t hat it is impossibl e to justify a corre la-tio n between rep roducti on of a training set and generali zation err or off of t he training set usin g only a pr iori reasoning. As a resu lt , the use in t he real world of any genera lizer t hat fits a hypothesis functi on to a training set (e.g., the use of back-propagation) is implicitl y pr edic at ed on an ass umpt ion abo ut the physical universe. This pap er shows how this ass umpt ion can be expressed in te rms of a non-Euclidean inn er product between two vectors, one represent ing t he ph ysical uni verse a nd on e representing t he generalizer. In deriving this result , a novel formalism for ad dress ing mac hine learni ng is develop ed. T his new formalism can be viewed as an exte nsion of t he conventional "Bayesian" formalism , to (among other t hings). allow one to address t he case in which one's assumed "priors" are not exactly correct. The most im-p or ta nt fea ture of this new formalism is t hat it uses an ext remely low-level event space, consis ting of triples of {\{}target fun ction , hypothesis fun cti on , train ing set {\}}. P artly as a resu lt of this fea ture, most other form alisms that have been constructed to address machine lea rn ing (e.g., PAC , the Bayesian formalism , and th e "st a tist ical mechanics" for malism) are sp ecial cases of t he form alism presented in this paper. Consequent ly such for malisms are capable of addressin g only a subset of t he issues ad dress ed in this pap er. In fact , the formalism of t his paper can be used to address all generalization issues of which the author is aware: overt ra in ing , the need to restrict the number of free para meters in t he hypothesis funct ion , th e problems associated wit h a "non-represent a tive" training set , whether and when cross-validat ion work s, wh ether and when stacked gene ra lizat ion work s, whe the r a nd wh en a particu lar regu lari zer will work , and so for th. A sum mary of som e of the more im port ant resu lt s of t his pap er conce rn ing t hese and related topi cs can be found in the conclusion .},
author = {Wolpert, David H},
booktitle = {Com p lex Systems},
file = {::},
pages = {47--94},
title = {{On the Connection between In-sample Testing and Generalization Error}},
url = {https://pdfs.semanticscholar.org/041c/670177d16bb2b2af80f5835b29de92666764.pdf},
volume = {6},
year = {1992}
}
@techreport{Dauphin,
abstract = {A central challenge to many fields of science and engineering involves minimizing non-convex error functions over continuous, high dimensional spaces. Gradient descent or quasi-Newton methods are almost ubiquitously used to perform such minimizations, and it is often thought that a main source of difficulty for these local methods to find the global minimum is the proliferation of local minima with much higher error than the global minimum. Here we argue, based on results from statistical physics, random matrix theory, neural network theory, and empirical evidence, that a deeper and more profound difficulty originates from the proliferation of saddle points, not local minima, especially in high dimensional problems of practical interest. Such saddle points are surrounded by high error plateaus that can dramatically slow down learning, and give the illusory impression of the existence of a local minimum. Motivated by these arguments, we propose a new approach to second-order optimization, the saddle-free Newton method, that can rapidly escape high dimensional saddle points, unlike gradient descent and quasi-Newton methods. We apply this algorithm to deep or recurrent neural network training, and provide numerical evidence for its superior optimization performance.},
author = {Dauphin, Yann N and Pascanu, Razvan and Gulcehre, Caglar and Cho, Kyunghyun and Ganguli, Surya and Bengio, Yoshua},
file = {::},
title = {{Identifying and attacking the saddle point problem in high-dimensional non-convex optimization}},
url = {https://papers.nips.cc/paper/5486-identifying-and-attacking-the-saddle-point-problem-in-high-dimensional-non-convex-optimization.pdf}
}
@techreport{Ge2015,
abstract = {We analyze stochastic gradient descent for optimizing non-convex functions. In many cases for non-convex functions the goal is to find a reasonable local minimum, and the main concern is that gradient updates are trapped in saddle points. In this paper we identify strict saddle property for non-convex problem that allows for efficient optimization. Using this property we show that stochastic gradient descent converges to a local minimum in a polynomial number of iterations. To the best of our knowledge this is the first work that gives global convergence guarantees for stochastic gradient descent on non-convex functions with exponentially many local minima and saddle points. Our analysis can be applied to orthogonal tensor decomposition, which is widely used in learning a rich class of latent variable models. We propose a new optimization formulation for the tensor decomposition problem that has strict saddle property. As a result we get the first online algorithm for orthogonal tensor decomposition with global convergence guarantee.},
archivePrefix = {arXiv},
arxivId = {1503.02101v1},
author = {Ge, Rong and Huang, Furong and Jin, Chi and Yuan, Yang},
eprint = {1503.02101v1},
file = {::},
keywords = {()},
title = {{Escaping From Saddle Points-Online Stochastic Gradient for Tensor Decomposition}},
url = {https://arxiv.org/pdf/1503.02101.pdf},
year = {2015}
}
@article{Ng2000,
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Ng, Andrew},
doi = {10.1111/j.1466-8238.2009.00506.x},
eprint = {arXiv:1011.1669v3},
file = {::},
isbn = {0226206807},
issn = {02660830},
journal = {CS229 Lecture notes},
number = {1},
pages = {1--3},
pmid = {21889629},
title = {{CS229 Lecture notes}},
url = {http://cs229.stanford.edu/notes/cs229-notes1.pdf http://www.stanford.edu/class/cs229/},
volume = {1},
year = {2000}
}
@misc{lasagne,
author = {Dieleman, Sander and Schl{\"{u}}ter, Jan and Raffel, Colin and Olson, Eben and S{\o}nderby, S{\o}ren Kaae and Nouri, Daniel and Maturana, Daniel and Thoma, Martin and Battenberg, Eric and Kelly, Jack and Fauw, Jeffrey De and Heilman, Michael and de Almeida, Diogo Moitinho and McFee, Brian and Weideman, Hendrik and Tak{\'{a}}cs, G{\'{a}}bor and de Rivaz, Peter and Crall, Jon and Sanders, Gregory and Rasul, Kashif and Liu, Cong and French, Geoffrey and Degrave, Jonas},
doi = {10.5281/zenodo.27878},
month = {aug},
title = {{Lasagne: First release.}},
url = {http://dx.doi.org/10.5281/zenodo.27878},
year = {2015}
}
@misc{EatonJohn2017,
author = {{Eaton, John}, W.},
doi = {10.3169/itej.65.790},
issn = {1342-6907},
title = {{Octave}},
url = {https://www.gnu.org/software/octave/ http://ci.nii.ac.jp/naid/110009669121/ http://jlc.jst.go.jp/DN/JST.JSTAGE/itej/65.790?lang=en{\&}from=CrossRef{\&}type=abstract},
year = {2017}
}
@techreport{Wolpert1997,
abstract = {A framework is developed to explore the connection between effective optimization algorithms and the problems they are solving. A number of "no free lunch" (NFL) theorems are presented which establish that for any algorithm, any elevated performance over one class of problems is offset by performance over another class. These theorems result in a geometric interpretation of what it means for an algorithm to be well suited to an optimization problem. Applications of the NFL theorems to information-theoretic aspects of optimization and benchmark measures of performance are also presented. Other issues addressed include time-varying optimization problems and a priori "head-to-head" minimax distinctions between optimization algorithms, distinctions that result despite the NFL theorems' enforcing of a type of uniformity over all algorithms.},
author = {Wolpert, David H and Macready, William G},
booktitle = {IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION},
file = {::},
keywords = {Index Terms-Evolutionary algorithms,information theory,optimization},
number = {1},
pages = {67},
title = {{No Free Lunch Theorems for Optimization}},
url = {https://ti.arc.nasa.gov/m/profile/dhw/papers/78.pdf},
volume = {1},
year = {1997}
}
@misc{chollet2015keras,
author = {Chollet, Fran{\c{c}}ois and Others},
howpublished = {$\backslash$url{\{}https://keras.io{\}}},
title = {{Keras}},
year = {2015}
}
@incollection{Bottoua,
abstract = {An abstract is not available.},
author = {Bottou, L{\'{e}}on},
booktitle = {On-Line Learning in Neural Networks},
doi = {10.1017/CBO9780511569920.003},
file = {::},
isbn = {978-0521117913},
issn = {0009-2673},
pages = {9--42},
title = {{On-line Learning and Stochastic Approximations}},
url = {https://www.cambridge.org/core/product/identifier/CBO9780511569920A009/type/book{\_}part}
}
@techreport{Bengio2012,
abstract = {Learning algorithms related to artificial neural networks and in particular for Deep Learning may seem to involve many bells and whistles, called hyper-parameters. This chapter is meant as a practical guide with recommendations for some of the most commonly used hyper-parameters, in particular in the context of learning algorithms based on back-propagated gradient and gradient-based optimization. It also discusses how to deal with the fact that more interesting results can be obtained when allowing one to adjust many hyper-parameters. Overall, it describes elements of the practice used to successfully and efficiently train and debug large-scale and often deep multi-layer neural networks. It closes with open questions about the training difficulties observed with deeper architectures.},
archivePrefix = {arXiv},
arxivId = {1206.5533v2},
author = {Bengio, Yoshua},
eprint = {1206.5533v2},
file = {::},
keywords = {()},
title = {{Practical Recommendations for Gradient-Based Training of Deep Architectures}},
url = {http://deeplearning.net/software/pylearn2},
year = {2012}
}
@article{Brieman2001,
abstract = {Random forests are a combination of tree predictors such that each tree depends on the values of a random vector sampled independently and with the same distribution for all trees in the forest. The generalization error for forests converges a.s. to a limit as the number of trees in the forest becomes large. The generalization error of a forest of tree classifiers depends on the strength of the individual trees in the forest and the correlation between them. Using a random selection of features to split each node yields error rates that compare favorably to Adaboost (Y. Freund {\&} R. Schapire, Machine Learning: Proceedings of the Thirteenth International conference, * * * , 148-156), but are more robust with respect to noise. Internal estimates monitor error, strength, and correlation and these are used to show the response to increasing the number of features used in the splitting. Internal estimates are also used to measure variable importance. These ideas are also applicable to regression.},
author = {Brieman, Leo},
file = {::},
journal = {Machine learning},
keywords = {classification,ensemble,regression},
pages = {5--32},
title = {{Random Forests}},
url = {https://link.springer.com/content/pdf/10.1023/A:1010933404324.pdf https://onlinelibrary.wiley.com/doi/abs/10.1002/9781118445112.stat06520},
volume = {45},
year = {2014}
}
@incollection{LeCunBOM12,
author = {LeCun, Yann and Bottou, L{\'{e}}on and Orr, Genevieve B and M{\"{u}}ller, Klaus-Robert},
booktitle = {Neural Networks: Tricks of the Trade (2nd ed.)},
editor = {Montavon, Gr{\'{e}}goire and Orr, Genevieve B and M{\"{u}}ller, Klaus-Robert},
isbn = {978-3-642-35288-1},
keywords = {BackProp Efficient},
pages = {9--48},
publisher = {Springer},
series = {Lecture Notes in Computer Science},
title = {{Efficient BackProp.}},
url = {http://dblp.uni-trier.de/db/series/lncs/lncs7700.html{\#}LeCunBOM12},
volume = {7700},
year = {2012}
}
@misc{matlab,
title = {{Unsupervised Learning - MATLAB {\&} Simulink}},
url = {https://www.mathworks.com/discovery/unsupervised-learning.html},
urldate = {2017-09-08}
}
@techreport{DeCock2011,
abstract = {This paper presents a data set describing the sale of individual residential property in Ames, Iowa from 2006 to 2010. The data set contains 2930 observations and a large number of explanatory variables (23 nominal, 23 ordinal, 14 discrete, and 20 continuous) involved in assessing home values. I will discuss my previous use of the Boston Housing Data Set and I will suggest methods for incorporating this new data set as a final project in an undergraduate regression course.},
author = {{De Cock}, Dean},
booktitle = {Journal of Statistics Education},
file = {::},
keywords = {Assessed Value,Group Project,Linear Models,Multiple Regression},
number = {3},
title = {{Ames, Iowa: Alternative to the Boston Housing Data as an End of Semester Regression Project}},
url = {www.amstat.org/publications/jse/v19n3/decock.pdf},
volume = {19},
year = {2011}
}
@article{Bivens1986,
author = {Bivens, Irl},
file = {::},
journal = {The College Mathematics Journal},
number = {2},
pages = {133--143},
title = {{What a tangent line is when it isn't a limit}},
url = {https://www.maa.org/sites/default/files/pdf/upload{\_}library/22/Polya/07468342.di020721.02p01112.pdf},
volume = {17},
year = {1986}
}
@article{GOOD1953,
abstract = {A random sample is drawn from a population of animals of various species. (The theory may also be applied to studies of literary vocabulary, for example.) If a particular species is represented r times in the sample of size N, then r/N is not a good estimate of the population frequency, p, when r is small. Methods are given for estimating p, assuming virtually nothing about the underlying population. The estimates are expressed in terms of smoothed values of the numbers nr (r= 1, 2, 3, ...), where nr is the number of distinct species that are each represented r times in the sample. (nr may be described as ‘the frequency of the frequency r'.) Turing is acknowledged for the most interesting formula in this part of the work. An estimate of the proportion of the population represented by the species occurring in the sample is an immediate corollary. Estimates are made of measures of heterogeneity of the population, including Yule's ‘characteristic' and Shannon's ‘entropy'. Methods are then discussed that do depend on assumptions about the underlying population. It is here that most work has been done by other writers. It is pointed out that a hypothesis can give a good fit to the numbers nr but can give quite the wrong value for Yule's characteristic. An example of this is Fisher's fit to some data of Williams's on Macrolepidoptera.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {GOOD, I. J.},
doi = {10.2307/2333344},
eprint = {arXiv:1011.1669v3},
file = {::},
isbn = {9788578110796},
issn = {00063444},
journal = {Biometrika},
number = {3-4},
pages = {237--264},
pmid = {25246403},
title = {{THE POPULATION FREQUENCIES OF SPECIES AND THE ESTIMATION OF POPULATION PARAMETERS}},
url = {http://pimedios.es/wp-content/uploads/2013/06/GoodTuring1953.pdf https://academic.oup.com/biomet/article-lookup/doi/10.1093/biomet/40.3-4.237},
volume = {40},
year = {1953}
}
@article{5392560,
abstract = {Two machine-learning procedures have been investigated in some detail using the game of checkers. Enough work has been done to verify the fact that a computer can be programmed so that it will learn to play a better game of checkers than can be played by the person who wrote the program. Furthermore, it can learn to do this in a remarkably short period of time (8 or 10 hours of machine-playing time) when given only the rules of the game, a sense of direction, and a redundant and incomplete list of parameters which are thought to have something to do with the game, but whose correct signs and relative weights are unknown and unspecified. The principles of machine learning verified by these experiments are, of course, applicable to many other situations.},
author = {Samuel, A L},
doi = {10.1147/rd.33.0210},
issn = {0018-8646},
journal = {IBM Journal of Research and Development},
month = {jul},
number = {3},
pages = {210--229},
title = {{Some Studies in Machine Learning Using the Game of Checkers}},
volume = {3},
year = {1959}
}
@article{Finnoff1993ImprovingMS,
author = {Finnoff, William and Hergert, Ferdinand and Zimmermann, Hans-Georg},
journal = {Neural Networks},
pages = {771--783},
title = {{Improving model selection by nonconvergent methods}},
volume = {6},
year = {1993}
}
@techreport{Mcgregor2006,
abstract = {Schumacher, Vose {\&} Whitley [1] have shown that Wolpert {\&} MacReady's celebrated No Free Lunch theorem [2] applies only to classes of target functions which are closed under permutation (c.u.p.). In the same paper, Schumacher et al. demonstrated that there exist both highly compressible and highly incompressible classes of objective functions for which NFL applies. However, I will show that there is a free lunch for the class of all n-compressible target functions f : X → Y given reasonable conditions on n, |X | and |Y|. While previous authors [3, 4] have considered NFL in the context of some form of complexity restriction on function classes, this paper appears to be the first to contain a proof using the general measure of Kolmogorov complexity. 1. NO FREE LUNCH When evolutionary algorithms were first introduced, it was hoped that they might provide a general-purpose "black box" search/optimisation tool. However, in [2], Wolpert {\&} MacReady proved that the average performance of all search algorithms, considered over the class of all possible target functions, is the same. Consequently evolutionary computing methods are "no better" than random search when considered over all possible fitness functions. It was subsequently proved in [1] that this "No Free Lunch" (NFL) result extends to classes of function other than the uniform class. NFL holds in the average case, regardless of the algorithm performance measure used, if and only if the class of functions under consideration is closed under permutation (c.u.p.). The lack of structure of c.u.p. classes has information-theoretic implications which have been explored in [5, 6] using Shannon-Weaver information theory; this paper investigates that lack of structure from an algorithmic information theory perspective using Kolmogorov complexity. I conclude that if we consider the class of algorithmically non-random (i.e. compressible) target functions, NFL does not hold. of largely theoretical interest to researchers using artificial evolution. This is due to a number of practical considerations such as local correlation structure in fitness spaces [7] and NFL assumptions ignoring algorithmic complexity [8]. The result presented in this paper is similarly theoretical rather than practical; nevertheless, it extends previously published work. 2. ASSUMPTIONS As in most previous NFL work, it is assumed that the search domain X and the set of objective values Y are sets of finite cardinality. Because the terms will be used frequently, I will use X = |X | and Y = |Y| to denote the cardinality of X and Y respectively. To avoid the trivial case where NFL necessarily holds, I will assume that Y ≥ 2. Additionally, it will be assumed that X ≫ 1 and that X ≥ Y. This is justified by the fact that in most practical search tasks the set of possible values of a target function is represented as a single fixed-width integer or floating-point number whilst the set of possible solutions is far larger. It is also justified in any case where target values are only used to rank possible solutions relative to one another, because the number of possible ranks is no larger than the number of possible solutions. 3. COMPRESSIBILITY 3.},
author = {Mcgregor, Simon},
file = {::},
title = {{No Free Lunch and Algorithmic Randomness}},
year = {2006}
}
@techreport{Wolpert1997a,
abstract = {A framework is developed to explore the connection between effective optimization algorithms and the problems they are solving. A number of "no free lunch" (NFL) theorems are presented which establish that for any algorithm, any elevated performance over one class of problems is offset by performance over another class. These theorems result in a geometric interpretation of what it means for an algorithm to be well suited to an optimization problem. Applications of the NFL theorems to information-theoretic aspects of optimization and benchmark measures of performance are also presented. Other issues addressed include time-varying optimization problems and a priori "head-to-head" minimax distinctions between optimization algorithms, distinctions that result despite the NFL theorems' enforcing of a type of uniformity over all algorithms.},
author = {Wolpert, David H and Macready, William G},
booktitle = {IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION},
file = {::},
keywords = {Index Terms-Evolutionary algorithms,information theory,optimization},
number = {1},
pages = {67},
title = {{No Free Lunch Theorems for Optimization}},
volume = {1},
year = {1997}
}
@book{Cormen2001,
archivePrefix = {arXiv},
arxivId = {2010 (ret. 29.4.2010)},
author = {Cormen, Thomas H and Leiserson, Charles E and Rivest, Ronald L and Stein, Clifford and London, Massachusetts and Company, Mcgraw-hill Book and Ridge, Boston Burr},
doi = {10.2307/2583667},
eprint = {2010 (ret. 29.4.2010)},
file = {::},
isbn = {0262032937},
issn = {15580768},
keywords = {0262033844,0262533057,9780262033848,9780262533058},
pmid = {22929350},
title = {{Introduction to Algorithms, Second Edition}},
url = {https://labs.xjtudlc.com/labs/wldmt/reading list/books/Algorithms and optimization/Introduction to Algorithms.pdf},
volume = {7},
year = {2001}
}
@techreport{Bottou,
abstract = {Chapter 1 strongly advocates the stochastic back-propagation method to train neural networks. This is in fact an instance of a more general technique called stochastic gradient descent (SGD). This chapter provides background material, explains why SGD is a good learning algorithm when the training set is large, and provides useful recommendations.},
author = {Bottou, L{\'{e}}on},
file = {::},
title = {{Stochastic Gradient Descent Tricks}},
url = {http://leon.bottou.org}
}
@article{Lagaris2008,
abstract = {We present three new stopping rules for Multistart based methods. The first uses a device that enables the determination of the coverage of the bounded search domain. The second is based on the comparison of asymptotic expectation values of observable quantities to the actually measured ones. The third offers a probabilistic estimate for the number of local minima inside the search domain. Their performance is tested and compared to that of other widely used rules on a host of test problems in the framework of Multistart.},
author = {Lagaris, I.E. and Tsoulos, I.G.},
doi = {10.1016/j.amc.2007.08.001},
file = {::},
issn = {00963003},
journal = {Applied Mathematics and Computation},
keywords = {multistart,stochastic global optimization,stopping rules},
number = {2},
pages = {622--632},
title = {{Stopping rules for box-constrained stochastic global optimization}},
url = {http://www.optimization-online.org/DB{\_}FILE/2007/07/1726.pdf http://linkinghub.elsevier.com/retrieve/pii/S0096300307008193},
volume = {197},
year = {2008}
}
@misc{Turing1939,
author = {Turing, A M},
file = {::},
title = {{Scientific Commons: Systems of Logic Based on Ordinals}},
url = {https://pure.mpg.de/rest/items/item{\_}2403325/component/file{\_}2403324/content http://en.scientificcommons.org/50288104},
year = {1939}
}
@article{JamesHastie2013,
abstract = {Review From the reviews: .,."There are interesting and non-standard topics that are not usually included in a first course in measture-theoretic probability including Markov Chains and MCMC, the bootstrap, limit theorems for martingales and mixing sequences, Brownian motion and Markov processes. The material is well-suported with many end-of-chapter problems." D.L. McLeish for Short Book Reviews of the ISI, December 2006 "The reader sees not only how measure theory is used to develop probability theory, but also how probability theory is used in applications. a The discourse is delivered in a theorem proof format and thus is better suited for classroom a . The authors prose is generally well thought out a . will make an attractive choice for a two-semester course on measure and probability, or as a second course for students with a semester of measure or probability theory under their belt." (Peter C. Kiessler, Journal of the American Statistical Association, Vol. 102 (479), 2007) "The book is a well written self-contained textbook on measure and probability theory. It consists of 18 chapters. Every chapter contains many well chosen examples and ends with several problems related to the earlier developed theory (some with hints). a At the very end of the book there is an appendix collecting necessary facts from set theory, calculus and metric spaces. The authors suggest a few possibilities on how to use their book." (Kazimierz Musial, Zentralblatt MATH, Vol. 1125 (2), 2008) "The title of the book consists of the names of its two basic parts. The booka (TM)s third part is comprised of some special topics from probability theory. a The authors suggest using the book intwo-semester graduate programs in statistics or a one-semester seminar on special topics. The material of the book is standard a is clear, comprehensive and a {\~{}}without being intimidatinga (TM)." (Rimas NorvaiAa, Mathematical Reviews, Issue 2007 f) Product Description This is a graduate level textbook on measure theory and probability theory. The book can be used as a text for a two semester sequence of courses in measure theory and probability theory, with an option to include supplemental material on stochastic processes and special topics. It is intended primarily for first year Ph.D. students in mathematics and statistics although mathematically advanced students from engineering and economics would also find the book useful. Prerequisites are kept to the minimal level of an understanding of basic real analysis concepts such as limits, continuity, differentiability, Riemann integration, and convergence of sequences and series. A review of this material is included in the appendix. The book starts with an informal introduction that provides some heuristics into the abstract concepts of measure and integration theory, which are then rigorously developed. The first part of the book can be used for a standard real analysis course for both mathematics and statistics Ph.D. students as it provides full coverage of topics such as the construction of Lebesgue-Stieltjes measures on real line and Euclidean spaces, the basic convergence theorems, L p spaces, signed measures, Radon-Nikodym theorem, Lebesgue's decomposition theorem and the fundamental theorem of Lebesgue integration on R, product spaces and product measures, and Fubini-Tonelli theorems. It also provides an elementary introduction to Banach and Hilbert spaces, convolutions, Fourier series and Fourier and Plancherel transforms. Thus part I would be particularly useful for students in a typical Statistics Ph.D. program if a separate course on real analysis is not a standard requirement. Part II (chapters 6-13) provides full coverage of standard graduate level probability theory. It starts with Kolmogorov's probability model and Kolmogorov's existence theorem. It then treats thoroughly the laws of large numbers including renewal theory and ergodic theorems with applications and then weak convergence of probability distributions, characteristic functions, the Levy-Cramer continuity theorem and the central limit theorem as well as stable laws. It ends with conditional expectations and conditional probability, and an introduction to the theory of discrete time martingales. Part III (chapters 14-18) provides a modest coverage of discrete time Markov chains with countable and general state spaces, MCMC, continuous time discrete space jump Markov processes, Brownian motion, mixing sequences, bootstrap methods, and branching processes. It could be used for a topics/seminar course or as an introduction to stochastic processes. From the reviews: "...There are interesting and non-standard topics that are not usually included in a first course in measture-theoretic probability including Markov Chains and MCMC, the bootstrap, limit theorems for martingales and mixing sequences, Brownian motion and Markov processes. The material is well-suported with many end-of-chapter problems." D.L. McLeish for Short Book Reviews of the ISI, December 2006},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Gareth, James and Daniela, Witten and Trevor, Hastie and Tibshirani, Robert},
doi = {10.1016/j.peva.2007.06.006},
eprint = {arXiv:1011.1669v3},
isbn = {9780387781884},
issn = {01621459},
pmid = {10911016},
title = {{An Introduction to Statistical Learning}},
url = {https://www-bcf.usc.edu/{~}gareth/ISL/ http://books.google.com/books?id=9tv0taI8l6YC},
volume = {8},
year = {2013}
}
@article{Raschka2018,
abstract = {The correct use of model evaluation, model selection, and algorithm selection techniques is vital in academic machine learning research as well as in many industrial settings. This article reviews different techniques that can be used for each of these three subtasks and discusses the main advantages and disadvantages of each technique with references to theoretical and empirical studies. Further, recommendations are given to encourage best practices in research and applications of machine learning. Common methods such as the holdout method for model evaluation and selection are covered, which are not recommended when working with small datasets. Different flavors of the bootstrap technique are introduced for estimating the uncertainty of performance estimates, as an alternative to confidence intervals via normal approximation if bootstrapping is computationally feasible. In the context of discussing the bias-variance trade-off, leave-one-out cross-validation is compared to k-fold cross-validation, and practical tips for the optimal choice of k are given based on empirical evidence. Different statistical tests for algorithm com-parisons are presented, and strategies for dealing with multiple comparisons such as omnibus tests and multiple-comparison corrections are discussed. Finally, alter-native methods for algorithm selection, such as 5×2cv cross-validation and nested cross-validation, are recommended for comparing machine learning algorithms when datasets are small.},
archivePrefix = {arXiv},
arxivId = {1811.12808},
author = {Raschka, Sebastian},
eprint = {1811.12808},
file = {::},
isbn = {9780511563928},
number = {January},
pages = {1--13},
title = {{Model evaluation , model selection , and algorithm selection in machine learning Performance Estimation : Generalization Performance Vs . Model Selection}},
url = {https://arxiv.org/pdf/1811.12808.pdf https://sebastianraschka.com/pdf/manuscripts/model-eval.pdf},
year = {2018}
}
@book{Oliphant2006,
author = {Oliphant, Travis},
title = {{Guide to NumPy}},
year = {2006}
}
@book{hastie01statisticallearning,
abstract = {The area's standard text revised and expanded. During the past decade has been an explosion in computation and information technology. With it has come vast amounts of data in a variety of fields such as medicine, biology, finance, and marketing. The challenge of understanding these data has led to the development of new tools in the field of statistics, and spawned new areas such as data mining, machine learning, and bioinformatics. Many of these tools have common underpinnings but are often expressed with different terminology. This book descibes the important ideas in these areas in a common conceptual framework. While the approach is statistical, the emphasis is on concepts rather than mathematics. Many examples are given, with a liberal use of color graphics. It should be a valuable resource for statisticians and anyone interested in data mining in science or industry. The book's coverage is broad, from supervised learning (prediction) to unsupervised learning. The many topics include neural networks, support vector machines, classification trees and boosting--the first comprehensive treatment of this topic in any book. This major new edition features many topics not covered in the original, including graphical models, random forests, ensemble methods, least angle regression {\&} path algorithms for the lasso, non-negative matrix factorization and spectral clustering. There is also a chapter on methods for ``wide'' data ( p bigger than n), including multiple testing and false discovery rates. Trevor Hastie, Robert Tibshirani, and Jerome Friedman are professors of statistics at Stanford University. They are prominent researchers in this area: Hastie and Tibshirani developed generalized additive models and wrote a popular book of that title. Hastie wrote much of the statistical modeling software in S-PLUS and invented principal curves and surfaces. Tibshirani proposed the Lasso and is co-author of the very successful An Introduction to the Bootstrap. Friedman is the co-inventor of many data-mining tools including CART, MARS, and projection pursuit.},
address = {New York, NY, USA},
author = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
publisher = {Springer New York Inc.},
series = {Springer Series in Statistics},
title = {{The Elements of Statistical Learning}},
year = {2001}
}
@article{Samuel1959,
author = {Samuel, AL},
doi = {10.1147/rd.33.0210},
file = {::},
isbn = {0018-8646},
issn = {0018-8646},
journal = {IBM Journal of research and development},
number = {3},
pages = {210--229},
title = {{Some studies in machine learning using the game of checkers}},
url = {https://pdfs.semanticscholar.org/e9e6/bb5f2a04ae30d8ecc9287f8b702eedd7b772.pdf http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=5392560},
volume = {3},
year = {1959}
}
@misc{wiki:gradient_descent,
annote = {[Online; accessed 27-April-2019]},
author = {Wikipedia contributors},
title = {{Gradient descent --- Wikipedia The Free Encyclopedia}},
url = {https://en.wikipedia.org/w/index.php?title=Gradient{\_}descent{\&}oldid=893231138},
year = {2019}
}
@techreport{Darken1992,
abstract = {Stochastic gradient descent is a general algorithm that includes LMS, on-line backpropagation, and adaptive k-means clustering as special cases. The standard choices of the learning rate (both adap-tive and dxed functions of time) often perform quite poorly. In contrast, our recently proposed class of $\backslash$search then converge" (STC) learning rate schedules (Darken and Moody, 1990b, 1991) display the theoretically optimal asymptotic convergence rate and a superior ability to escape from poor local minima However, the user is responsible for setting a key parameter. We propose here a new methodology for creating the rst automatically adapting learning rates that achieve the optimal rate of convergence.},
author = {Darken, Christian and Chang, Joseph and Moody, John},
file = {::},
publisher = {IEEE Press, 445 Hoes Lane},
title = {{LEARNING RATE SCHEDULES FOR FASTER STOCHASTIC GRADIENT SEARCH}},
url = {https://pdfs.semanticscholar.org/9db5/54243d7588589569aea127d676c9644d069a.pdf},
year = {1992}
}
@misc{DeCock,
author = {{De Cock}, Dean},
title = {{House Prices: Advanced Regression Techniques | Kaggle}},
url = {https://www.kaggle.com/c/house-prices-advanced-regression-techniques{\#}description https://www.kaggle.com/jimthompson/house-prices-advanced-regression-techniques/ensemble-model-stacked-model-example/notebook},
urldate = {2017-09-16}
}
@techreport{DuchiJDUCHI2011,
abstract = {We present a new family of subgradient methods that dynamically incorporate knowledge of the geometry of the data observed in earlier iterations to perform more informative gradient-based learning. Metaphorically, the adaptation allows us to find needles in haystacks in the form of very predictive but rarely seen features. Our paradigm stems from recent advances in stochastic optimization and online learning which employ proximal functions to control the gradient steps of the algorithm. We describe and analyze an apparatus for adaptively modifying the proximal function , which significantly simplifies setting a learning rate and results in regret guarantees that are provably as good as the best proximal function that can be chosen in hindsight. We give several efficient algorithms for empirical risk minimization problems with common and important regu-larization functions and domain constraints. We experimentally study our theoretical analysis and show that adaptive subgradient methods outperform state-of-the-art, yet non-adaptive, subgradient algorithms.},
author = {{Duchi JDUCHI}, John and Singer, Yoram},
booktitle = {Journal of Machine Learning Research},
file = {::},
keywords = {adaptivity,online learning,stochastic convex optimization,subgradient methods},
pages = {2121--2159},
title = {{Adaptive Subgradient Methods for Online Learning and Stochastic Optimization * Elad Hazan}},
url = {http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf},
volume = {12},
year = {2011}
}
@article{Dozat,
abstract = {This work aims to improve upon the recently proposed and rapidly popular-ized optimization algorithm Adam (Kingma {\&} Ba, 2014). Adam has two main components—a momentum component and an adaptive learning rate component. However, regular momentum can be shown conceptually and empirically to be in-ferior to a similar algorithm known as Nesterov's accelerated gradient (NAG). We show how to modify Adam's momentum component to take advantage of insights from NAG, and then we present preliminary evidence suggesting that making this substitution improves the speed of convergence and the quality of the learned mod-els.},
author = {Dozat, Timothy},
doi = {10.1016/j.ahj.2004.02.019},
file = {::},
issn = {00028703},
journal = {International Conference on Learning Representations (ICLR) Workshop},
number = {1},
pages = {2013--2016},
title = {{Incorporating Nesterov Momentum into Adam}},
url = {http://mattmahoney.net/dc/text8.zip https://openreview.net/pdf?id=OM0jvwB8jIp57ZJjtNEZ},
year = {2016}
}
@book{savov2017no,
abstract = {Linear algebra is the foundation of science and engineering. Knowledge of linear algebra is a prerequisite for studying statistics, machine learning, computer graphics, signal processing, chemistry, economics, quantum mechanics, and countless other applications. Indeed, linear algebra offers a powerful toolbox for modelling the real world. The NO BULLSHIT guide to LINEAR ALGEBRA shows the connections between the computational techniques of linear algebra, their geometric interpretations, and the theoretical foundations. This university-level textbook contains lessons on linear algebra written in a style that is precise and concise. Each concept is illustrated through definitions, formulas, diagrams, explanations, and examples of real-world applications. Readers build their math superpowers by solving practice problems and learning to use the computer algebra system SymPy to speed up tedious matrix arithmetic tasks.},
author = {Savov, I},
isbn = {9780992001025},
publisher = {Minireference Publishing},
title = {{No Bullshit Guide to Linear Algebra}},
url = {https://books.google.com/books?id=A4WzswEACAAJ},
year = {2017}
}
@misc{wiki:supervisedlearning,
annote = {[Online; accessed 
8-September-2017
]},
author = {Wikipedia},
title = {{Supervised learning --- Wikipedia{\{},{\}} The Free Encyclopedia}},
url = {https://en.wikipedia.org/w/index.php?title=Supervised{\_}learning{\&}oldid=791408094},
year = {2017}
}
@techreport{Zeiler,
abstract = {We present a novel per-dimension learning rate method for gradient descent called ADADELTA. The method dynamically adapts over time using only first order information and has minimal computational overhead beyond vanilla stochas-tic gradient descent. The method requires no manual tuning of a learning rate and appears robust to noisy gradient information , different model architecture choices, various data modalities and selection of hyperparameters. We show promising results compared to other methods on the MNIST digit classification task using a single machine and on a large scale voice dataset in a distributed cluster environment.},
archivePrefix = {arXiv},
arxivId = {1212.5701v1},
author = {Zeiler, Matthew D},
eprint = {1212.5701v1},
file = {::},
keywords = {Gradient Descent,Index Terms-Adaptive Learning Rates,Machine Learn-ing,Neural Networks},
title = {{ADADELTA: AN ADAPTIVE LEARNING RATE METHOD}},
url = {https://arxiv.org/pdf/1212.5701.pdf}
}
@misc{Bradford2017,
abstract = {Machine learning is exploding, with smart algorithms being used everywhere from email to smartphone apps to marketing campaigns. Translation: if you're looking for an in-demand career, setting yourself up with the skills to work with smart machines/artificial intelligence is a good move.

With input from Florian Douetteau, CEO of Dataiku, here are some things you can start doing today to position yourself for a future career in machine learning.},
author = {Bradford, Laurence},
booktitle = {Forbes},
title = {{8 Ways You Can Succeed In A Machine Learning Career}},
url = {https://www.forbes.com/sites/laurencebradford/2017/07/28/8-ways-you-can-succeed-in-a-machine-learning-career/{\#}1b0e26bf3c32},
urldate = {2019-03-31},
year = {2017}
}
@misc{James2017,
author = {James, Gareth and Witten, Daniela and Hastie, Trevor and Maintainer, Rob Tibshirani},
file = {::},
title = {{Data for an Introduction to Statistical Learning with Applications in R}},
url = {http://www.liacs.nl/{~}putten/library/cc2000/},
year = {2017}
}
@misc{Ng,
abstract = {Machine learning is the science of getting computers to act without being explicitly programmed. In the past decade, machine learning has given us self-driving cars, practical speech recognition, effective web search, and a vastly improved understanding of the human genome. Machine learning is so pervasive today that you probably use it dozens of times a day without knowing it. Many researchers also think it is the best way to make progress towards human-level AI. In this class, you will learn about the most effective machine learning techniques, and gain practice implementing them and getting them to work for yourself. More importantly, you'll learn about not only the theoretical underpinnings of learning, but also gain the practical know-how needed to quickly and powerfully apply these techniques to new problems. Finally, you'll learn about some of Silicon Valley's best practices in innovation as it pertains to machine learning and AI. This course provides a broad introduction to machine learning, datamining, and statistical pattern recognition. Topics include: (i) Supervised learning (parametric/non-parametric algorithms, support vector machines, kernels, neural networks). (ii) Unsupervised learning (clustering, dimensionality reduction, recommender systems, deep learning). (iii) Best practices in machine learning (bias/variance theory; innovation process in machine learning and AI). The course will also draw from numerous case studies and applications, so that you'll also learn how to apply learning algorithms to building smart robots (perception, control), text understanding (web search, anti-spam), computer vision, medical informatics, audio, database mining, and other areas.},
author = {Ng, Andrew},
title = {{Machine Learning by Stanford University}},
url = {https://www.coursera.org/learn/machine-learning/home/welcome},
urldate = {2017-09-08},
year = {2015}
}
@article{jia2014caffe,
author = {Jia, Yangqing and Shelhamer, Evan and Donahue, Jeff and Karayev, Sergey and Long, Jonathan and Girshick, Ross and Guadarrama, Sergio and Darrell, Trevor},
journal = {arXiv preprint arXiv:1408.5093},
title = {{Caffe: Convolutional Architecture for Fast Feature Embedding}},
year = {2014}
}
@inproceedings{DBLP:journals/corr/KingmaB14,
author = {Kingma, Diederik P and Ba, Jimmy},
booktitle = {ICLR},
title = {{Adam: {\{}A{\}} Method for Stochastic Optimization}},
url = {http://arxiv.org/abs/1412.6980},
year = {2015}
}
@inproceedings{atakulreka2007,
abstract = {Feedforward neural networks are particularly useful in learning a training dataset without prior knowledge. However, weight adjusting with a gradient descent may result in the local minimum problem. Repeated training with random starting weights is among the popular methods to avoid this problem, but it requires extensive computational time. This paper proposes a simultaneous training method with removal criteria to eliminate less promising neural networks, which can decrease the probability of achieving a local minimum while efficiently utilizing resources. The experimental results demonstrate the effectiveness and efficiency of the proposed training method in comparison with conventional training.},
address = {Berlin, Heidelberg},
author = {Atakulreka, Akarachai and Sutivong, Daricha},
booktitle = {AI 2007: Advances in Artificial Intelligence},
editor = {Orgun, Mehmet A and Thornton, John},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Atakulreka, Sutivong - Unknown - Avoiding Local Minima in Feedforward Neural Networks by Simultaneous Learning.pdf:pdf},
isbn = {978-3-540-76928-6},
keywords = {Feedforward Neural Networks,Local Minima,Removal Criteria,Simultaneous Learning},
pages = {100--109},
publisher = {Springer Berlin Heidelberg},
title = {{Avoiding Local Minima in Feedforward Neural Networks by Simultaneous Learning}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.2375{\&}rep=rep1{\&}type=pdf},
year = {2007}
}
@inproceedings{Li2014,
abstract = {Stochastic gradient descent (SGD) is a popular technique for large-scale optimization problems in machine learning. In order to parallelize SGD, minibatch training needs to be employed to reduce the communication cost. However, an increase in minibatch size typically decreases the rate of convergence. This paper introduces a technique based on approximate optimization of a conservatively regular-ized objective function within each minibatch. We prove that the convergence rate does not decrease with increasing minibatch size. Experiments demonstrate that with suitable implementations of approximate optimization, the resulting algorithm can outperform standard SGD in many scenarios.},
archivePrefix = {arXiv},
arxivId = {1206.5533},
author = {Li, Mu and Zhang, Tong and Chen, Yuqiang and Smola, Alexander J},
booktitle = {Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining - KDD '14},
doi = {10.1145/2623330.2623612},
eprint = {1206.5533},
file = {::},
isbn = {9781450329569},
issn = {03029743},
pages = {661--670},
pmid = {25497547},
title = {{Efficient mini-batch training for stochastic optimization}},
url = {http://dx.doi.org/10.1145/2623330.2623612 http://dl.acm.org/citation.cfm?doid=2623330.2623612},
year = {2014}
}
@article{Varma2006,
abstract = {BACKGROUND: Cross-validation (CV) is an effective method for estimating the prediction error of a classifier. Some recent articles have proposed methods for optimizing classifiers by choosing classifier parameter values that minimize the CV error estimate. We have evaluated the validity of using the CV error estimate of the optimized classifier as an estimate of the true error expected on independent data. RESULTS: We used CV to optimize the classification parameters for two kinds of classifiers; Shrunken Centroids and Support Vector Machines (SVM). Random training datasets were created, with no difference in the distribution of the features between the two classes. Using these "null" datasets, we selected classifier parameter values that minimized the CV error estimate. 10-fold CV was used for Shrunken Centroids while Leave-One-Out-CV (LOOCV) was used for the SVM. Independent test data was created to estimate the true error. With "null" and "non null" (with differential expression between the classes) data, we also tested a nested CV procedure, where an inner CV loop is used to perform the tuning of the parameters while an outer CV is used to compute an estimate of the error. The CV error estimate for the classifier with the optimal parameters was found to be a substantially biased estimate of the true error that the classifier would incur on independent data. Even though there is no real difference between the two classes for the "null" datasets, the CV error estimate for the Shrunken Centroid with the optimal parameters was less than 30{\{}{\%}{\}} on 18.5{\{}{\%}{\}} of simulated training data-sets. For SVM with optimal parameters the estimated error rate was less than 30{\{}{\%}{\}} on 38{\{}{\%}{\}} of "null" data-sets. Performance of the optimized classifiers on the independent test set was no better than chance. The nested CV procedure reduces the bias considerably and gives an estimate of the error that is very close to that obtained on the independent testing set for both Shrunken Centroids and SVM classifiers for "null" and "non-null" data distributions. CONCLUSION: We show that using CV to compute an error estimate for a classifier that has itself been tuned using CV gives a significantly biased estimate of the true error. Proper use of CV for estimating true error of a classifier developed using a well defined algorithm requires that all steps of the algorithm, including classifier parameter tuning, be repeated in each CV loop. A nested CV procedure provides an almost unbiased estimate of the true error.},
author = {Varma, Sudhir and Simon, Richard},
doi = {10.1186/1471-2105-7-91},
file = {::},
isbn = {1471-2105},
issn = {14712105},
journal = {BMC Bioinformatics},
pmid = {16504092},
title = {{Bias in error estimation when using cross-validation for model selection}},
url = {http://www.biomedcentral.com/1471-2105/7/91},
volume = {7},
year = {2006}
}
@techreport{Piech2016,
author = {Piech, Chris},
file = {::},
title = {{Logistic Regression}},
year = {2016}
}
@inproceedings{j.2018on,
author = {Reddi, Sashank J and Kale, Satyen and Kumar, Sanjiv},
booktitle = {International Conference on Learning Representations},
title = {{On the Convergence of Adam and Beyond}},
url = {https://openreview.net/forum?id=ryQu7f-RZ},
year = {2018}
}
@misc{Leek,
abstract = {One of the most common tasks performed by data scientists and data analysts are prediction and machine learning. This course will cover the basic components of building and applying prediction functions with an emphasis on practical applications. The course will provide basic grounding in concepts such as training and tests sets, overfitting, and error rates. The course will also introduce a range of model based and algorithmic machine learning methods including regression, classification trees, Naive Bayes, and random forests. The course will cover the complete process of building prediction functions including data collection, feature creation, algorithms, and evaluation.},
author = {Leek, Jeff PhD and Peng, Roger D. PhD and {Caffo, Brian}, PhD},
title = {{Coursera | Practical Machine Learning | Data Science Specialization by Johns Hopkins University}},
url = {https://www.coursera.org/learn/practical-machine-learning},
urldate = {2017-09-08}
}
@techreport{Roughgarden2018,
abstract = {Gradient descent is an extremely simple algorithm-simpler than most of the algorithms you studied in CS161-that has been around for centuries. These days, the main "killer app" is machine learning. Model-fitting often reduces to optimization-for example, maximizing the likelihood of observed data over a family of generative models. A remarkably large fraction of modern machine learning research, including some of the much-hyped recent work on "deep learning," boils down to implementing variants of gradient descent on a very large scale (i.e., for huge training sets). Indeed, the choice of models in many machine learning applications is driven as much by computational considerations-whether or not gradient descent can be implemented quickly-as by any other criteria. The first goal of these notes is to develop the geometry and intuition behind gradient descent, to the point that the algorithm seems totally obvious. The second goal is to make the general method concrete with a case study on linear regression. (The method is also very useful for many other problems.) Throughout these notes, you might want to keep Figure 1 in mind. The figures show a succession of lines that are an increasingly good fit for a collection of points in the plane. "Linear regression" just means computing the best-fitting line, and this succession of lines is generated by successive iterations of gradient descent. This is not all supposed to make 100{\%} sense yet, of course-the rest of the notes explains what's going on-but this example should provide you with a concrete picture to refer back to as the lecture proceeds. *},
author = {Roughgarden, Tim and Valiant, Gregory},
file = {::},
title = {{CS168: The Modern Algorithmic Toolbox Gradient Descent Basics}},
url = {https://web.stanford.edu/class/cs168/l/grad{\_}desc{\_}notes.pdf},
year = {2018}
}
@techreport{Lee2016,
abstract = {We show that gradient descent converges to a local minimizer, almost surely with random initializa-tion. This is proved by applying the Stable Manifold Theorem from dynamical systems theory.},
archivePrefix = {arXiv},
arxivId = {1602.04915v2},
author = {Lee, Jason D and Simchowitz, Max and {Jordan ♯ †}, Michael I and Recht, Benjamin},
eprint = {1602.04915v2},
file = {::},
keywords = {()},
title = {{Gradient Descent Converges to Minimizers}},
url = {https://arxiv.org/pdf/1602.04915.pdf},
year = {2016}
}
@misc{Dayan1999,
abstract = {Unsupervised learning studies how systems can learn to represent particular input patterns in a way that reflects the statistical structure of the overall collection of input patterns. By contrast with SUPERVISED LEARNING or REINFORCEMENT LEARNING, there are no explicit target outputs or environmental evaluations associated with each input; rather the unsupervised learner brings to bear prior biases as to what aspects of the structure of the input should be captured in the output.},
author = {Dayan, Peter},
booktitle = {The MIT Encyclopedia of the Cognitive Sciences},
doi = {10.1016/j.visres.2007.07.023},
issn = {0042-6989},
keywords = {Animals,Cats,Learning,Learning: physiology,Models,Neurological,Neuronal Plasticity,Neuronal Plasticity: physiology,Neurons,Neurons: physiology,Orientation,Orientation: physiology,Photic Stimulation,Photic Stimulation: methods,Sensory Deprivation,Sensory Deprivation: physiology,Visual Cortex,Visual Cortex: physiology,Visual Perception,Visual Perception: physiology},
number = {22},
pages = {1--29},
pmid = {17850840},
title = {{Unsupervised Learning}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/17850840},
urldate = {2017-09-08},
volume = {47},
year = {1999}
}
@misc{Nga,
abstract = {Machine learning is the science of getting computers to act without being explicitly programmed. In the past decade, machine learning has given us self-driving cars, practical speech recognition, effective web search, and a vastly improved understanding of the human genome. Machine learning is so pervasive today that you probably use it dozens of times a day without knowing it. Many researchers also think it is the best way to make progress towards human-level AI. In this class, you will learn about the most effective machine learning techniques, and gain practice implementing them and getting them to work for yourself. More importantly, you'll learn about not only the theoretical underpinnings of learning, but also gain the practical know-how needed to quickly and powerfully apply these techniques to new problems. Finally, you'll learn about some of Silicon Valley's best practices in innovation as it pertains to machine learning and AI.},
author = {Ng, Andrew},
title = {{Machine Learning | Coursera}},
url = {https://www.coursera.org/learn/machine-learning/home/info},
urldate = {2017-09-16}
}
@article{Ruder2016,
archivePrefix = {arXiv},
arxivId = {arXiv:1609.04747v2},
author = {Ruder, Sebastian},
eprint = {arXiv:1609.04747v2},
file = {:C$\backslash$:/Users/John/Documents/Data Science/Resources/An overview of gradient descent optimization.pdf:pdf},
pages = {1--14},
title = {{An overview of gradient descent optimization algorithms}},
year = {2016}
}
@article{Iizuka2003,
abstract = {Background: Hepatocellular carcinoma has a poor prognosis because of the high intrahepatic recurrence rate. There are technological limitations to traditional methods such as TNM staging for accurate prediction of recurrence, suggesting that new techniques are needed. Methods: We investigated mRNA expression profiles in tissue specimens from a training set, comprising 33 patients with hepatocellular carcinoma, with high-density oligonucleotide microarrays representing about 6000 genes. We used this training set in a supervised learning manner to construct a predictive system, consisting of 12 genes, with the Fisher linear classifier. We then compared the predictive performance of our system with that of a predictive system with a support vector machine (SVM-based system) on a blinded set of samples from 27 newly enrolled patients. Findings: Early intrahepatic recurrence within 1 year after curative surgery occurred in 12 (36{\%}) and eight (30{\%}) patients in the training and blinded sets, respectively. Our system correctly predicted early intrahepatic recurrence or non-recurrence in 25 (93{\%}) of 27 samples in the blinded set and had a positive predictive value of 88{\%} and a negative predictive value of 95{\%}. By contrast, the SVM-based system predicted early intrahepatic recurrence or non-recurrence correctly in only 16 (60{\%}) individuals in the blinded set, and the result yielded a positive predictive value of only 38{\%} and a negative predictive value of 79{\%}. Interpretation: Our system predicted early intrahepatic recurrence or non-recurrence for patients with hepatocellular carcinoma much more accurately than the SVM-based system, suggesting that our system could serve as a new method for characterising the metastatic potential of hepatocellular carcinoma.},
author = {Iizuka, Norio and Oka, Masaaki and Yamada-Okabe, Hisafumi and Nishida, Minekatsu and Maeda, Yoshitaka and Mori, Naohide and Takao, Takashi and Tamesa, Takao and Tangoku, Akira and Tabuchi, Hisahiro and Hamada, Kenji and Nakayama, Hironobu and Ishitsuka, Hideo and Miyamoto, Takanobu and Hirabayashi, Akira and Uchimura, Shunji and Hamamoto, Yoshihiko},
doi = {10.1016/S0140-6736(03)12775-4},
file = {::},
isbn = {0140-6736 LA - eng PT - Journal Article},
issn = {01406736},
journal = {Lancet},
number = {9361},
pages = {923--929},
pmid = {12648972},
title = {{Oligonucleotide microarray for prediction of early intrahepatic recurrence of hepatocellular carcinoma after curative resection}},
url = {www.thelancet.com},
volume = {361},
year = {2003}
}
